---
title: "STAT 5104 HW 7"
author: "Samantha Sunshine"
date: "10/25/17"
output:
  html_notebook: default
---

```{r setup, include=FALSE, eval=T}
  #rm(list=ls())
  graphics.off()
  options(scipen = 1, digits = 6, stringsAsFactors = F)
  knitr::opts_chunk$set(echo = TRUE, eval= TRUE, cache = F, tidy = TRUE,
            include = TRUE, message = F, warning = F,
            tidy.opts=list(width.cutoff=55))
  library.warn <- library
  library <- function(package, help, pos = 2, lib.loc = NULL, character.only = FALSE,
                logical.return = FALSE, warn.conflicts = FALSE, quietly = TRUE,
                verbose = getOption("verbose")) {
               if (!character.only) {
                  package <- as.character(substitute(package))
               }
               suppressPackageStartupMessages(library.warn(
                  package, help, pos, lib.loc, character.only = TRUE,
                  logical.return, warn.conflicts, quietly, verbose))}
    library(foreach)
    library(doParallel)
    library(doRNG)
    library(parallel)
```

# Problem 2

In this problem, we are to play around with the *parallel* package to get a feel for how this works as we scale our compute.  The compute is rather simple in that we are computing sums of squares error.  Not a real computational challenge, so the focus is on getting the parallel compute to work.

First, we need some data:
```{r problem2_data}
set.seed(12345)
y <- rnorm(n = 1000, mean = 1, sd = 1)
```

Ok, now that we have the data, lets do the compute using:  

* a regular for loop  
* using vector math  
* using `%dopar%`  
* using `parSapply`  

We will keep track of the time in each realizing that the overhead of moving the data around in this simple example may negate the benefit of using more than one core on our computer.  I should note a good tutorial on this here:  

<http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/>

```{r problem2_code, eval=T}
#for loop to calculate the sum of squares total for y
ybar <- as.vector(mean(y))
SST <- 0
time_ssa <- system.time({
  for(i in 1:length(y)){
    SST <- SST + (y[i] - ybar)^2
  }
})

#vectors to calculate the sum of squares total for y
ssy <- sum((y-ybar)^2)
w <- 0
time_ssb <- system.time({
  w <- t(y-ybar) %*% (y-ybar)
})

#dopar to calculate the sum of squares total for y
cl <- makeCluster(3)
registerDoParallel(cl)
time_ssc <- system.time({
  ss_dopar <- foreach(i=1:length(y), .combine = "+") %dopar% {
    sum((y[i] - ybar)^2)
  }
})
stopCluster(cl)

#parSapply to calculate the sum of squares total for y
ss_sap <- function(i){
  return((i - ybar)^2)
}
cl <- makeCluster(3)
clusterExport(cl, varlist=c("ss_sap","ybar"))
time_ssd <- system.time({
  ss_parS <- sum(parSapply(cl, y, function(i) ss_sap(i), USE.NAMES = F, simplify = T))
})
stopCluster(cl)
```

(a) The for loop calculated a sum of squared error of `r SST`.  

(b) The vector multiplication calculated a sum of squared error of `r w`.  

(c) The dopar function calculated a sum of squared error of `r ss_dopar`. I set the parameter in the foreach loop where i=1:length(y), and ybar was the mean of y. I also used the .combine function to add everything in my loop.  

(d) The parSapply function is awesome.  I will use it all the time.  The ss error is calculated as `r ss_parS`.  

$~$  

Table 1: Sum of Squared Error  

```{r, echo = FALSE}
#table of sums of squared error
sse <- data.frame(SST, w, ss_dopar, ss_parS)
round_sse <- round(sse, digits = 4)
rownames(round_sse) <- "SSE"
knitr::kable(round_sse, col.names = c("Part A: for loop", "Part B: vectors", "Part C: dopar", "Part D: parSapply"))
```


$~$  
Table 2: Times for Sum of Squared Error  
```{r, echo = FALSE}
#table of times for sums of squared error
time_table1 <- rbind(time_ssa,time_ssb,time_ssc,time_ssd)
rownames(time_table1) <- c("Part A", "Part B", "Part C", "Part D")
knitr::kable(time_table1[,1:3])
```

I even ran the system time multiple times and it still came up with the same times. We can see from the times that the vector multiplication was the fastest, followed by the for loop, and finally the dopar function.  Perhaps next time I will use the microbenchmark function to run the simulation several times to get some error bars on my system.  I should scale up to what the problem has asked, but for now, I am off to a test tomorrow.

# Problem 3

In this problem, we are asked to parallelize the gradient descent algorithm from the previous homework.  The hint suggested we parallelize around parameters we had to provide.  There were two parameters we had to provide: initial values for the parameters and alpha.  I am going to leave alpha alone and provide a grid of points as initial values.

For the gradient descent algorithm parallelization, I used the parSapply function so that I could create a function of x that replicated the algorithm from the previous gradient descent problem. I named my function grad_des, and I used this function inside parSapply. I made a cluster and exported the cluster before the parSapply function, and I stopped the cluster following the function. The results from the gradiant descent are below, along with the coefficients from the linear model function.

The code for this is as follows:  

```{r problem3_code}
#generate the data
set.seed(1256)
theta <- as.matrix(c(1,2), nrow =2)
X <- cbind(1, rep(1:10,10))
h <- X %*% theta + rnorm(100,0,0.2)

#new and current thetas pre-allocated, choosing values for alpha, tolerance (t), and m as length of h
theta_current <- as.matrix(c(0,0), nrow =2)
theta_new <- as.matrix(c(1,1), nrow =2)
alpha <- 0.0001
t <- 0.000001
m <- length(h)

#gradient descent, this is great for a single run through the params
#what if I wanted to try several initial values for theta_1
#  
# I should also add a fail safe to the algorithm, IE a max iterations
tX <- t(X)
t5 <- system.time({
while(sum(abs(theta_new-theta_current)>t)){
  theta_current <- theta_new
  theta_grad <- tX %*% ((X %*% theta_current) - h)
  theta_new <- theta_current - alpha/m * theta_grad
  }
})

#parallelized gradient descent
grad_des <- function(k){
    theta_current <- theta_seeds[,k]
    theta_new <- theta_current + 10*t
    i <- 0
    while(sum(abs(theta_new-theta_current)>t) & i<1e6){
      i <- i + 1
      theta_current <- theta_new
      theta_grad <- tX %*% ((X %*% theta_current) - h)
      theta_new <- theta_current - alpha/m * theta_grad
      }
    return(theta_new)
}
cl_3 <- makeCluster(3)
theta_seeds <- matrix(rbind(rep(0,6),0:5), nrow =2)
clusterExport(cl_3, varlist=c("grad_des", "t", "tX", "X", "alpha", "m", "h","theta_seeds"))

time_gd <- system.time({
  final_thetas <- parSapply(cl_3, 1:ncol(theta_seeds), function(k) grad_des(k), USE.NAMES = F, simplify = T)
})
stopCluster(cl_3)

#linear model of original gradient descent
lmgrad <- coef(lm(h~0+X))
```



Table 3: Gradient Descent Values  
```{r, echo = FALSE}
#table of gradient descent values
colnames(final_thetas) <- paste0("theta1_start=",0:5)
rownames(final_thetas) <- c("theta0","theta1")
knitr::kable(final_thetas)
```

Table 4: Coefficients of Linear Model  
```{r, echo=FALSE}
#lm coefficients
lmgrad <- data.frame(beta0=lmgrad[1],beta1=lmgrad[2])
knitr::kable(lmgrad, row.names = F)
```

As you can see in Tables 3 and 4, the coefficients using both methods are very similar. Further, in THIS case, initial values of $\Theta_1$ result in very similar final values.  I SHOULD add the iterator to the return value to see how different that is based on how far the initial value is from the actual.

# Problem 4

```{r, echo = FALSE}
#generating data
set.seed(1267)
n <- 200
X <- 1/cbind(1, rt(n, df = 1), rt(n, df = 1), rt(n, df = 1))
beta <- c(1, 2, 3, 0)
Y <- X %*% beta + rnorm(100, sd = 3)

#Bootstrap method
B <- 10000
bootbeta <- matrix(0, nrow=B, ncol=5)
bootid <- matrix(0, nrow=B, ncol=length(beta))
time_boot1 <- system.time({
  for(i in 1:B){
    bootid <- sample(1:n, n, replace = T)
    boot_x <- X[bootid,1:4]
    boot_y <- Y[bootid, 1]
    bootbeta[i,] <- coef(lm(boot_y~boot_x))
  }
})

#parallelizing bootstrap
cl_2 <- makeCluster(3)
registerDoParallel(cl_2)
time_boot2 <- system.time({
  foreach(i=1:B) %dopar% {
    bootid <- sample(1:n, n, replace = T)
    boot_x <- X[bootid,1:4]
    boot_y <- Y[bootid, 1]
    bootbeta[i,] <- coef(lm(boot_y~boot_x))
  }
})
stopCluster(cl_2)
```

(a) Algorithm for Bootstrap  

Using the bootstrap method for calculating $\hat{\beta}$ begins by pre-allocating a matrix for all $\hat{\beta}$ after being run for B=10,000 times. I also pre-allocated a matrix for the indexing of $\hat{\beta}$. The for loop sampled 200 numbers from X, and generated a new set of coefficients for the new x and y datasets.    

For the parallelization, I used the dopar function along with a foreach loop instead of the original for loop. The rest of the copmutation was the same, except for adding a cluster and registering doParallel before the loop, as well as stopping the cluster after the loop.  

(b) Tables of Results   
$~$  

Table 5: Linear Model Coefficients from Bootstrap (only rows 1 through 20)   
```{r, echo = FALSE}
#table of bootstrap results
knitr::kable(bootbeta[1:20, -2], caption = "Linear Model Coefficients from Bootstrap", col.names = c("Beta_1", "Beta_2", "Beta_3", "Beta_0"))
```

Table 6: Summary of Linear Model Coefficients from Bootstrap   
```{r, echo = FALSE}
#bootstrap results summary
knitr::kable(summary(bootbeta[, -2]), caption = "Summary of Linear Model Coefficients from Bootstrap", col.names = c("Beta_1", "Beta_2", "Beta_3", "Beta_0"))
```

The times of the original bootstrap method and the parallelized bootstrap method are very similar. A table of the times is shown below. The parallel time actually showed a longer elapsed time than the original bootstrap method.   
$~$  

Table 7: System Time from Bootstrap Methods  
```{r, echo = FALSE}
#table of bootstrap times
time_table2 <- rbind(c(9.14, 0.00, 9.17), c(3.61, 0.71, 9.46))
rownames(time_table2) <- c("Original", "Parallel")
knitr::kable(time_table2, col.names = c("user", "system", "elapsed"))
```


(c) Histograms of $\hat{\beta}$  

```{r, echo = FALSE}
#histograms of beta values from bootstrap
par(mfrow = c(2, 2))
hist(bootbeta[, 1], xlab = "Beta_1", main = "Figure 1: Histogram of Beta_1")
hist(bootbeta[, 3], xlab = "Beta_2", main = "Figure 2: Histogram of Beta_2")
hist(bootbeta[, 4], xlab = "Beta_3", main = "Figure 3: Histogram of Beta_3")
hist(bootbeta[, 5], xlab = "Beta_0", main = "Figure 4: Histogram of Beta_0")
```

In Figure 1, we can see that the $\beta_1$ values appear to follow some type of normal distribution because of the bell-shaped curve. Figures 2, 3, and 4 all have similar shapes, but do not resemble the normal distribution quite as well as Figure 1.  $\beta_3$ appears to have the highest mean around 3, while $\beta_0$ has the lowest mean around 0. 

# Appendix (Partial to illustrate how to do it ...)

```{r Appendix, ref.label=c("problem2_code","problem3_code"), echo=TRUE, eval=F, tidy=TRUE, include=T}

```